---
title: "Model Interpretability"
author: "Harriet O'Brien, Vy Tran, Saniyah Khan, Rehinatu Usman"
format: 
    html:
        toc: true
        toc-location: right
        number-sections: true
        theme: cosmo
        self-contained: true
---

# Introduction

The goal of this section is to understand how the final Gradient Boosting model make predictions and to check whether the model behavior aligns with the patterns observed during EDA.

Interpretability also helps confirm that the model is using features in a reasonable way and that its errors do not follow any strong pattern.

This section includes:

- Feature Importance
- Partial Dependence Plots
- Actual vs. Predicted
- Residual Analysis

These steps provide a clear overview of how reliable and transparent the final model is.


```{python}
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.inspection import PartialDependenceDisplay
```

# Load Processed Features and Recreate Trainâ€“Test Split

Here we reload the processed dataset and recreate the exact same train/test split as the previous modeling file.
This ensures the interpretability results match the final training conditions.

```{python}
# Load the final processed feature matrix

features = pd.read_csv("../data/features_processed.csv")

# Identify the correct target column (scaled version)

target_col = "reimbursement_amount"

# Recreate X and y

y = features[target_col]
X = features.drop(columns=[target_col])

# Recreate the same train-test split used in the model file

X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.2, random_state=42
)

# Re-train the two models for interpretability

rf = RandomForestRegressor(random_state=42)
rf.fit(X_train, y_train)

gb = GradientBoostingRegressor(random_state=42)
gb.fit(X_train, y_train)

X.head()

```
Random Forest provides feature importance scores based on how much each variable reduces prediction error across the trees.
This gives a simple but useful view of which features matter the most.

# Feature Importance

Feature importance tells us which input variables influenced predictions the most.

## Random Forest
```{python}
rf_importance = rf.feature_importances_
importance_df = pd.DataFrame({
"feature": X_train.columns,
"importance": rf_importance
}).sort_values(by="importance", ascending=False)

importance_df

```
```{python}
plt.figure(figsize=(8,5))
sns.barplot(data=importance_df, x="importance", y="feature")
plt.title("Random Forest Feature Importance")
plt.tight_layout()
plt.show()

```
Higher values mean the feature had a stronger effect on predictions.

Random Forest feature importance is shown here as a supporting comparison to check consistency across tree-based models. 

## Gradient Boosting Feature Importance
```{python}
gb_importance = gb.feature_importances_
gb_df = pd.DataFrame({
"feature": X_train.columns,
"importance": gb_importance
}).sort_values(by="importance", ascending=False)

gb_df

```
```{python}
plt.figure(figsize=(8,5))
sns.barplot(data=gb_df, x="importance", y="feature")
plt.title("Gradient Boosting Feature Importance")
plt.tight_layout()
plt.show()

```
Gradient Boosting usually focuses on fewer strong predictors.
If both models highlight similar features,this increases confidence in the stability of the learned relationships.

Because several features are derived from the same underlying variables such as raw and scaled receipt amounts, feature importance values should be interpreted in relation to one another rather than individually. 

# Partial Dependence Plots (PDPs)
PDPs help show how the prediction changes when we vary one feature while keeping other constant.
```{python}
features_to_plot = ["total_receipts_amount_scaled",
"miles_traveled_scaled",
"trip_duration_days"]

fig, ax = plt.subplots(figsize=(10,6))
PartialDependenceDisplay.from_estimator(
gb, X_train, features_to_plot, ax=ax
)
plt.tight_layout()
plt.show()

```
The PDPs show that predicted reimbursement increases clearly as total receipt amounts increase, confirming that receipts are the main factor of influencing reimbursement decisions. 

Mileage and trip duration exhibit more gradual, monotonic effects, indicating that they contribute to reimbursement but have a lesser of a role compared to receipts. 

These trends are consistent with observations made from the EDA and employee interviews, suggesting that the Gradient Boosting model is learning structured reimbursement rules rather than random or misleading relationships.

# Actual vs Predicted 
This tells us how close model predictions are to the real test values.
```{python}
gb_preds = gb.predict(X_test)

plt.figure(figsize=(6,6))
plt.scatter(y_test, gb_preds, alpha=0.5)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Gradient Boosting: Actual vs Predicted")
plt.grid(True)
plt.tight_layout()
plt.show()

```
Most points follow a diagonal pattern, showing that the predicted reimbursement values are generally close to the actual values. 

There is more variation for higher reimbursement amounts, which is expected for longer or more expensive trips. Overall, the Gradient Boosting model shows strong predictive performance without clear bias. 

# Residual Distribution + Residuals vs Predictions

Residuals help us check whether there is any bias or cosistent error pattern.
```{python}
residuals = y_test - gb_preds

plt.figure(figsize=(6,4))
sns.histplot(residuals, kde=True)
plt.title("Residual Distribution")
plt.tight_layout()
plt.show()

```
```{python}
plt.figure(figsize=(6,4))
plt.scatter(gb_preds, residuals, alpha=0.5)
plt.axhline(0, color="red", linestyle="--")
plt.xlabel("Predicted")
plt.ylabel("Residual")
plt.title("Residuals vs Predictions")
plt.tight_layout()
plt.show()

```
The residuals appear roughly centered around zero, with no strong bias pattern in the model's prediction.

A slightly wider spread for higher reimbursement values suggest increased uncertainty for extreme cases, which is reasonable given that in the real-world there's a variety of travel expenses. Overall, the residual analysis supports the reliability of the model. 

# Summary

Overall, the interpretability results are consistent with what we observed during EDA and modeling stages. The Gradient Boosting model relies heavily on receipt-related features, with trip duration and mileage providing play supporting roles, which matches how reimbursement decisions are expected to work.

Feature importance and partial dependence plots confirm that the model behaves logically and structured way, rather than fitting random noise in the data. The residual analysis also show no clear bias or error patterns. 

Together, these findings suggest that the final model is both accurate and easy to interpret, and that it provides a conjecture of how the legacy reimbursement system works and its behavior. 


