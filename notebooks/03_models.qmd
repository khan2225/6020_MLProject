---
title: "Modeling Reimbursement Amount"
author: "Harriet O'Brien, Vy Tran, Saniyah Khan, Rehinatu Usman"
format: 
    html:
        toc: true
        number-sections: true
        theme: cosmo
execute:
    python:
        reticulate: false
---

# Introduction

In this section, we build predictive models to estimate the reimbursement amount based on the engineered features created earlier. Our goal is to evaluate multiple regression models, compare their performance, and identify which algorithm provides the most accurate and stable predictions. We include three models: **Linear Regression, Random Forest, and Gradient Boosting**; and use standard metrics such as MAE, RMSE, and R^2 for evaluation.
We test multiple models to ensure that the strong baseline performance is not accidental and to confirm whether more flexible models offer any measurable improvement.



# Load Data
```{python}
# Basic set up
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor

# Load processed features 
features = pd.read_csv("../data/features_processed.csv")

# Quick check to ensure the dataset loaded correctly
features.head()
features.shape
```

# Train-Test Split

We predict the original reimbursement amount using the engineered features. 

```{python}
# Split the dataset into training and testing sets
# We predict 'reimbursement_amount' using the engineered features.
X = features.drop(columns=["reimbursement_amount"])
y = features["reimbursement_amount"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```
```{python}
#| eval: false
# Linear Regression baseline model
# (Executed in terminal to avoid reticulate crash)
```

# Baseline Model: Linear Regression
```{python}
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Create and fit the model
lr = LinearRegression()
lr.fit(X_train, y_train)

# Make predictions on the test set
lr_preds = lr.predict(X_test)

# Evaluate model performance
lr_mae = mean_absolute_error(y_test, lr_preds)
lr_rmse = np.sqrt(mean_squared_error(y_test, lr_preds))
lr_r2 = r2_score(y_test, lr_preds)

# Clean formatted output
print("Linear Regression performance:")
print(f"  MAE : {lr_mae:.2f}")
print(f"  RMSE: {lr_rmse:.2f}")
print(f"  R²  : {lr_r2:.3f}")
```
Linear Regression provides a simple baseline by modeling a straight-line relationship between features and the target. This helps us understand whether the dataset is linearly predictable before testing more flexible models.

# Random Forest Model
```{python}
from sklearn.ensemble import RandomForestRegressor

# Create and train the model
rf = RandomForestRegressor(
    n_estimators=300,
    random_state=42
)
rf.fit(X_train, y_train)

# Predictions
rf_preds = rf.predict(X_test)

# Evaluation
rf_mae = mean_absolute_error(y_test, rf_preds)
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_preds))
rf_r2 = r2_score(y_test, rf_preds)

print("Random Forest performance:")
print(f"  MAE:  {rf_mae:.2f}")
print(f"  RMSE: {rf_rmse:.2f}")
print(f"  R²:   {rf_r2:.3f}")
```
Random Forest uses many decision trees and averages their predictions. This helps reduce variance and capture non-linear patterns that linear models may miss.

# Gradient Boosting Model
```{python}
from sklearn.ensemble import GradientBoostingRegressor

gbr = GradientBoostingRegressor(
    learning_rate=0.05,
    n_estimators=200,
    max_depth=3,
    random_state=42
)

gbr.fit(X_train, y_train)
gbr_preds = gbr.predict(X_test)

# Evaluation
gbr_mae = mean_absolute_error(y_test, gbr_preds)
gbr_rmse = np.sqrt(mean_squared_error(y_test, gbr_preds))
gbr_r2 = r2_score(y_test, gbr_preds)

print("Gradient Boosting performance:")
print(f"  MAE:  {gbr_mae:.2f}")
print(f"  RMSE: {gbr_rmse:.2f}")
print(f"  R²:   {gbr_r2:.3f}")

```
Gradient Boosting trains trees sequentially, where each new tree focuses on correcting the errors of the previous ones. This often leads to strong performance on structured datasets.

# Compare All Models In One Table
```{python}
results = pd.DataFrame({
    "Model": ["Linear Regression", "Random Forest", "Gradient Boosting"],
    "MAE":   [lr_mae, rf_mae, gbr_mae],
    "RMSE":  [lr_rmse, rf_rmse, gbr_rmse],
    "R²":    [lr_r2, rf_r2, gbr_r2]
})

results

```

# Visualization
```{python}
import matplotlib.pyplot as plt

plt.scatter(y_test, rf_preds, alpha=0.6)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Random Forest: Actual vs Predicted")
plt.grid(True)
plt.show()
```

# Model Evaluation Summary

Overall, all three models (Linear Regression, Random Forest, Gradient Boosting) achieve strong predictive performance, but on different levels. The tree-based models, particularly Gradient Boosting, show the highest accuracy with R^2 values around 0.97 and relatively low MAE and RMSE. Linear Regression performs reasonalbly well but does not capture as much of the variability as the ensemble models. This shows that the relationship between the engineered features and the reimbursement amount contains nonlinear patterns that the ensemble models are able to capture more effectively. 

Linear Regression provides a useful baseline, but its lower R^2 value (0.86) indicates that it cannot fully model the underlying complexity of the legacy reimbursement system. The improvement from Linear Regression to the tree-based models suggests that the system's rules are not purely linear and that more flexible models are needed to represent its behavior. 

Based on these findings, Gradient Boosting and Random Forest offer the most stable and accurate performance. Linear Regression is still valuable as a reference model, but the ensemble methods better reflect the structure and variablity present in the data. 

Although the models reach high accuracy, this does not gurantee that they fully reconstruct the undocumented business logic of the legacy system. Some of the strong performance may result from deterministic patterns in the engineered features, so interpretability checks are required to validate that the model's decisions align with domain expectations. 

Since the more complex models demonstrate noticeably better accuracy than Linear Regression, we treat Gradient Boosting and Random Forest as robust checks to confirm that the relationship between features and reimbursement amount is stable. Linear Regression remains useful as a simpler baseline, but the ensemble models better capture the underlying structure of the legacy system. 


 
# Note on polynomial/interaction features

We briefly considered adding polynomial or interaction terms (for example squared mileage or interactions between miles and receipts). However, the tree-based models already capture nonlinear patterns automatically, and both Random Forest and Gradient Boosting achieve high accuracy eithout requiring additional engineered nonlinear features.

While adding polynomial terms might help Linear Regression slightly, it would also increase model complexity and reduce interpretability. Since the ensemble models already learn these relationships effectively and outperform the linear model, introducing extra polynomial or interaction features was not necessary for achieving strong predictive performance. 

# Export Final Model
 
```{python}
import joblib
# choose final model (here Linear Regression because the results are equivalent to RF and GBM)
final_model = lr
joblib.dump(final_model, "../models/final_model.pkl")
print("Model saved as final_model.pkl")
```




