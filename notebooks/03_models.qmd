---
title: "Modeling Reimbursement Amount"
author: "Harriet O'Brien, Vy Tran, Saniyah Khan, Rehinatu Usman"
format: 
    html:
        toc: true
        number-sections: true
        theme: cosmo
---

#Introduction

In this section, we build predictive models to estimate the scaled trip duration based on the engineered features created earlier. Our goal is to evaluate multiple regression models, compare their performance, and identify which algorithm provides the most accurate and stable predictions. We include three models-linear Regression, Random Forest, and Gradient Boosting-and use standard metrics such as MAE, RMSE, and R^2 for evaluation.
We test multiple models to ensure that the strong baseline performance is not accidental and to comfirn whether more flexible models offer any measurable improvement.
# Setup
```{r}
library(reticulate)
use_python("C:/Users/tuong/anaconda3/envs/mlclean/python.exe", required = TRUE)
py_config()
```
# Load Data
```{python}
# Basic set up
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor

# Load processed features 
features = pd.read_csv("../data/features_processed.csv")

# Quick check to ensure the dataset loaded correctly
features.head()
features.shape
```
# Train-Test Split

We predict the engineered target variable trip_duration_days_scaled.

```{python}
# Split the dataset into training and testing sets
# We predict 'trip_duration_days_scaled' using the engineered features.
X = features.drop(columns=["trip_duration_days_scaled"])
y = features["trip_duration_days_scaled"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```
```{python}
#| eval: false
# Linear Regression baseline model
# (Executed in terminal to avoid reticulate crash)
```

# Baseline model: Linear Regression
```{python}
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Create and fit the model
lr = LinearRegression()
lr.fit(X_train, y_train)

# Make predictions on the test set
lr_preds = lr.predict(X_test)

# Evaluate model performance
lr_mae = mean_absolute_error(y_test, lr_preds)
lr_rmse = np.sqrt(mean_squared_error(y_test, lr_preds))
lr_r2 = r2_score(y_test, lr_preds)

# Clean formatted output
print("Linear Regression performance:")
print(f"  MAE : {lr_mae:.2f}")
print(f"  RMSE: {lr_rmse:.2f}")
print(f"  R²  : {lr_r2:.3f}")
```
Linear Regression provides a simple baseline by modeling a straight-line relationship between features and the target. This helps us understand whether the dataset is linearly predictable before testing more flexible models.

# Random Forest model
```{python}
from sklearn.ensemble import RandomForestRegressor

# Create and train the model
rf = RandomForestRegressor(
    n_estimators=300,
    random_state=42
)
rf.fit(X_train, y_train)

# Predictions
rf_preds = rf.predict(X_test)

# Evaluation
rf_mae = mean_absolute_error(y_test, rf_preds)
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_preds))
rf_r2 = r2_score(y_test, rf_preds)

print("Random Forest performance:")
print(f"  MAE:  {rf_mae:.2f}")
print(f"  RMSE: {rf_rmse:.2f}")
print(f"  R²:   {rf_r2:.3f}")
```
Random Forest uses many decision trees and averages their predictions. This helps reduce variance and capture non-linear patterns that linear models may miss.

# Gradient Boosting model
```{python}
from sklearn.ensemble import GradientBoostingRegressor

gbr = GradientBoostingRegressor(
    learning_rate=0.05,
    n_estimators=200,
    max_depth=3,
    random_state=42
)

gbr.fit(X_train, y_train)
gbr_preds = gbr.predict(X_test)

# Evaluation
gbr_mae = mean_absolute_error(y_test, gbr_preds)
gbr_rmse = np.sqrt(mean_squared_error(y_test, gbr_preds))
gbr_r2 = r2_score(y_test, gbr_preds)

print("Gradient Boosting performance:")
print(f"  MAE:  {gbr_mae:.2f}")
print(f"  RMSE: {gbr_rmse:.2f}")
print(f"  R²:   {gbr_r2:.3f}")

```
Gradient Boosting trains trees sequentially, where each new tree focuses on correcting the errors of the previous ones. This often leads to strong performance on structured datasets.

# Compare all models in one table
```{python}
results = pd.DataFrame({
    "Model": ["Linear Regression", "Random Forest", "Gradient Boosting"],
    "MAE":   [lr_mae, rf_mae, gbr_mae],
    "RMSE":  [lr_rmse, rf_rmse, gbr_rmse],
    "R²":    [lr_r2, rf_r2, gbr_r2]
})

results

```

# Visualization
```{python}
import matplotlib.pyplot as plt

plt.scatter(y_test, rf_preds, alpha=0.6)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Random Forest: Actual vs Predicted")
plt.grid(True)
plt.show()
```

All three models achieve near-perfect performance, with MAE and RMSE close to zero and R^2 equal to 1.0.
This outcome is expected because the engineered target variable is strongly determined by the created features and behaves smoothly after scaling. As a result, even simple models like Linear Regression can fit the data almost perfectly.
These results suggest that the engineered features fully explain the target variation, and the dataset is highly predictable. More complex models (RF, GBM) do not offer meaningful improvements, confirming that the underlying relationship is already very strong and mostly linear.