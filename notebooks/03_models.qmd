---
title: "Modeling Reimbursement Amount"
author: "Harriet O'Brien, Vy Tran, Saniyah Khan, Rehinatu Usman"
format: 
    html:
        toc: true
        number-sections: true
        theme: cosmo
execute:
    python:
        reticulate: false
---

# Introduction

In this section, we build predictive models to estimate the reimbursement amount based on the engineered features created earlier. Our goal is to evaluate multiple regression models, compare their performance, and identify which algorithm provides the most accurate and stable predictions. We include three models: **Linear Regression, Random Forest, and Gradient Boosting**; and use standard metrics such as MAE, RMSE, and R^2 for evaluation.
We test multiple models to ensure that the strong baseline performance is not accidental and to confirm whether more flexible models offer any measurable improvement.



# Load Data
```{python}
# Basic set up
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor

# Load processed features 
features = pd.read_csv("../data/features_processed.csv")

# Quick check to ensure the dataset loaded correctly
features.head()
features.shape
```

# Train-Test Split

We predict the original reimbursement amount using the engineered features. 

```{python}
# Split the dataset into training and testing sets
# We predict 'reimbursement_amount' using the engineered features.
X = features.drop(columns=["reimbursement_amount"])
y = features["reimbursement_amount"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```
```{python}
#| eval: false
# Linear Regression baseline model
# (Executed in terminal to avoid reticulate crash)
```

# Baseline Model: Linear Regression
```{python}
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Create and fit the model
lr = LinearRegression()
lr.fit(X_train, y_train)

# Make predictions on the test set
lr_preds = lr.predict(X_test)

# Evaluate model performance
lr_mae = mean_absolute_error(y_test, lr_preds)
lr_rmse = np.sqrt(mean_squared_error(y_test, lr_preds))
lr_r2 = r2_score(y_test, lr_preds)

# Clean formatted output
print("Linear Regression performance:")
print(f"  MAE : {lr_mae:.2f}")
print(f"  RMSE: {lr_rmse:.2f}")
print(f"  R²  : {lr_r2:.3f}")
```
Linear Regression provides a simple baseline by modeling a straight-line relationship between features and the target. This helps us understand whether the dataset is linearly predictable before testing more flexible models.

# Random Forest Model
```{python}
from sklearn.ensemble import RandomForestRegressor

# Create and train the model
rf = RandomForestRegressor(
    n_estimators=300,
    random_state=42
)
rf.fit(X_train, y_train)

# Predictions
rf_preds = rf.predict(X_test)

# Evaluation
rf_mae = mean_absolute_error(y_test, rf_preds)
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_preds))
rf_r2 = r2_score(y_test, rf_preds)

print("Random Forest performance:")
print(f"  MAE:  {rf_mae:.2f}")
print(f"  RMSE: {rf_rmse:.2f}")
print(f"  R²:   {rf_r2:.3f}")
```
Random Forest uses many decision trees and averages their predictions. This helps reduce variance and capture non-linear patterns that linear models may miss.

# Gradient Boosting Model
```{python}
from sklearn.ensemble import GradientBoostingRegressor

gbr = GradientBoostingRegressor(
    learning_rate=0.05,
    n_estimators=200,
    max_depth=3,
    random_state=42
)

gbr.fit(X_train, y_train)
gbr_preds = gbr.predict(X_test)

# Evaluation
gbr_mae = mean_absolute_error(y_test, gbr_preds)
gbr_rmse = np.sqrt(mean_squared_error(y_test, gbr_preds))
gbr_r2 = r2_score(y_test, gbr_preds)

print("Gradient Boosting performance:")
print(f"  MAE:  {gbr_mae:.2f}")
print(f"  RMSE: {gbr_rmse:.2f}")
print(f"  R²:   {gbr_r2:.3f}")

```
Gradient Boosting trains trees sequentially, where each new tree focuses on correcting the errors of the previous ones. This often leads to strong performance on structured datasets.

# Compare All Models In One Table
```{python}
results = pd.DataFrame({
    "Model": ["Linear Regression", "Random Forest", "Gradient Boosting"],
    "MAE":   [lr_mae, rf_mae, gbr_mae],
    "RMSE":  [lr_rmse, rf_rmse, gbr_rmse],
    "R²":    [lr_r2, rf_r2, gbr_r2]
})

results

```

# Visualization
```{python}
import matplotlib.pyplot as plt

plt.scatter(y_test, rf_preds, alpha=0.6)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Random Forest: Actual vs Predicted")
plt.grid(True)
plt.show()
```

# Model Evaluation Summary

All three models demnostrate strong predictive performance, but clear differences emerge when comparing their accuracy and flexibility. Linear Regression performs resonably well as a baseline model, achieving an R^2 of approximately 0.86, but it does not capture the full complexity of the reimbursement data. 

The tree-based models, particularly Gradient Boosting, achieve higher accuracy with R^2 values around 0.97 and lower MAE and RMSE. This improvement indicates that the relationship between the engineered features and reimbursement amount is not purely linear and contains meaningful nonlinear patterns. 

Random Forest and Gradient Boosting both model these nonlinear relationships effectively. Among them, Gradient Boosting provides the most stable and accurate results, making it the best overall choice for approximating the legacy reimbursement system. 

While Linear Regression remains useful as a simple reference model, the performance of Gradient Boosting suggests that a more flexible approach is necessary to reflect the structure and rules embedded in the historical reimbursement data. Interpretability analysis is therefore used in the next section to ensure that the model's behavior aligns with the expections set.  

 
# Note on polynomial/interaction features

We briefly considered adding polynomial or interaction terms (for example squared mileage or interactions between miles and receipts). However, the tree-based models already capture nonlinear patterns automatically, and both Random Forest and Gradient Boosting achieve high accuracy eithout requiring additional engineered nonlinear features.

While adding polynomial terms might help Linear Regression slightly, it would also increase model complexity and reduce interpretability. Since the ensemble models already learn these relationships effectively and outperform the linear model, introducing extra polynomial or interaction features was not necessary for achieving strong predictive performance. 

# Export Final Model
 
```{python}
import joblib
# choose final model (here Gradient boosting due to performance)
final_model = gbr
joblib.dump(final_model, "../models/final_model.pkl")
print("Model saved as final_model.pkl")
```




